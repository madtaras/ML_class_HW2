{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Передбачення зарплат на IT-ринку України"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому завданні ви працюватимете з реальними даними з [зарплатного опитування DOU.ua за травень 2016р](https://dou.ua/lenta/articles/salary-report-may-june-2016/). Ви реалізуєте зважену лінійну регресію, яка передбачає зарплати Java-інженерів, та навчите свою модель за допомогою градієнтного спуску.\n",
    "\n",
    "Заповніть пропущений код в розділі «Моделювання» (позначено коментарями) та запустіть розділ «Тестування», щоб перевірити його правильність."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Підготовка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries = pd.read_csv(\"data/2016_may_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оберемо тільки Java-інженерів з-поміж усіх респондентів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java = pd.DataFrame(df_salaries[(df_salaries[\"Язык.программирования\"] == \"Java\") &\n",
    "                                   (df_salaries[\"cls\"] == \"DEV\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейменуємо деякі колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.rename(\n",
    "    columns={\n",
    "        \"exp\": \"TotalExperience\",\n",
    "        \"loc\": \"Location\"\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо рівень англійської мови числами від 1 (найнижчий) до 5 (найвищий):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java[\"EnglishLevel\"] = df_java[\"Уровень.английского\"].map({\n",
    "    \"элементарный\": 1,\n",
    "    \"ниже среднего\": 2,\n",
    "    \"средний\": 3,\n",
    "    \"выше среднего\": 4,\n",
    "    \"продвинутый\": 5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо колонку Location (найбільші IT-міста або \"other\") за допомогою one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_columns = [\n",
    "    \"LocationOther\",\n",
    "    \"LocationDnipro\",\n",
    "    \"LocationKyiv\",\n",
    "    \"LocationLviv\",\n",
    "    \"LocationOdesa\",\n",
    "    \"LocationKharkiv\"\n",
    "]\n",
    "df_java[city_columns] = pd.get_dummies(df_java[\"Location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відберемо такі ознаки:\n",
    "\n",
    "* Загальна кількість років досвіду\n",
    "* Рівень англійської мови\n",
    "* Місто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"TotalExperience\", \"EnglishLevel\"] + city_columns\n",
    "df_X = df_java[feature_columns]\n",
    "df_y = df_java[[\"salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (929, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", df_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalExperience</th>\n",
       "      <th>EnglishLevel</th>\n",
       "      <th>LocationOther</th>\n",
       "      <th>LocationDnipro</th>\n",
       "      <th>LocationKyiv</th>\n",
       "      <th>LocationLviv</th>\n",
       "      <th>LocationOdesa</th>\n",
       "      <th>LocationKharkiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TotalExperience  EnglishLevel  LocationOther  LocationDnipro  \\\n",
       "5               0.5             3              1               0   \n",
       "7               5.0             1              0               0   \n",
       "17              0.0             3              0               0   \n",
       "27              4.0             3              0               0   \n",
       "28              6.0             4              0               0   \n",
       "39              3.0             4              0               1   \n",
       "46              2.0             4              0               0   \n",
       "49              3.0             3              0               0   \n",
       "59              2.0             3              0               0   \n",
       "89              1.0             5              0               0   \n",
       "\n",
       "    LocationKyiv  LocationLviv  LocationOdesa  LocationKharkiv  \n",
       "5              0             0              0                0  \n",
       "7              1             0              0                0  \n",
       "17             1             0              0                0  \n",
       "27             1             0              0                0  \n",
       "28             1             0              0                0  \n",
       "39             0             0              0                0  \n",
       "46             0             0              0                1  \n",
       "49             1             0              0                0  \n",
       "59             0             0              0                1  \n",
       "89             1             0              0                0  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    salary\n",
       "5      500\n",
       "7     1600\n",
       "17     600\n",
       "27    3400\n",
       "28    2880\n",
       "39    1425\n",
       "46    1700\n",
       "49    1800\n",
       "59    1235\n",
       "89    1200"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розділимо вибірку на навчальну та тестову:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_assignment = np.random.uniform(size=len(df_X))\n",
    "\n",
    "X_train = df_X[dataset_assignment <= training_set_size].values\n",
    "y_train = df_y[dataset_assignment <= training_set_size].values.flatten()\n",
    "\n",
    "X_test = df_X[dataset_assignment > training_set_size].values\n",
    "y_test = df_y[dataset_assignment > training_set_size].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб градієнтний спуск швидше збігався, нормалізуємо навчальну вибірку так, щоб кожна ознака мала $\\mu = 0, \\sigma = 1$:\n",
    "\n",
    "$ x' = \\frac{x - \\bar{x}}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = np.average(X_train, axis=0)\n",
    "feature_sigmas = np.std(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - feature_means) / feature_sigmas\n",
    "X_test = (X_test - feature_means) / feature_sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додаємо уявну ознаку $x_0 = 1$ (intercept term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all(X_train[:, 0] == 1):\n",
    "    X_train = np.insert(X_train, 0, values=1, axis=1)\n",
    "    \n",
    "if not np.all(X_test[:, 0] == 1):\n",
    "    X_test = np.insert(X_test, 0, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:  (741, 9)\n",
      "y train:  (741,)\n",
      "\n",
      "X test:   (188, 9)\n",
      "y test:   (188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train: \", X_train.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print()\n",
    "print(\"X test:  \", X_test.shape)\n",
    "print(\"y test:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Моделювання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію гіпотези лінійної регресії в матричній формі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear(theta, X):\n",
    "    # Compute the hypothesis function for linear regression.\n",
    "    \n",
    "    return np.dot(X, theta)\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію зважування всіх навчальних прикладів $x^{(i)}$, якщо нам дана точка передбачення $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_weights(X, x_pred, tau):\n",
    "    # Compute the weight for each example, given the\n",
    "    # prediction point (x_pred).\n",
    "\n",
    "    weights = np.exp(np.sum(- ((X-x_pred) ** 2) / (2 * tau ** 2), axis=1))\n",
    "    # ====================================================\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(theta, X, y, weights):\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the overall loss on the training set (X),\n",
    "    # taking the weights into account.\n",
    "\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    result = (1.0 / (2*m)) * np.sum(weights * (predict_linear(theta, X).flatten() - y) ** 2)\n",
    "    \n",
    "    return result\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте обчислення градієнта функції втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_gradient(theta, X, y, weights):\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the gradient of the cost function on the\n",
    "    # training set (X), taking the weights into account.\n",
    "\n",
    "    m = X.shape[0]\n",
    "    result = (1.0 / m) * X.T.dot(weights * (np.dot(X, theta) - y))\n",
    "    \n",
    "    return result\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте один крок градієнтного спуску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_weights(theta, learning_rate, cost_gradient):\n",
    "    # Given the learning rate and the gradient of the\n",
    "    # cost function, take one gradient descent step and\n",
    "    # return the updated vector theta.\n",
    "    \n",
    "    theta = theta - learning_rate * cost_gradient\n",
    "    \n",
    "    return theta\n",
    "    # ===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Навчаємо модель:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weights, loss_fun, grad_fun, learning_rate, convergence_threshold, max_iters, verbose=False):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        loss = loss_fun(theta, X, y, weights)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: {0:3} Loss: {1}\".format(i + 1, loss))\n",
    "\n",
    "        if len(losses) > 2 and np.abs(losses[-1] - losses[-2]) <= convergence_threshold:\n",
    "            break\n",
    "        \n",
    "        grad = grad_fun(theta, X, y, weights)\n",
    "        theta = update_model_weights(theta, learning_rate, grad)\n",
    "        \n",
    "    return theta, np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Передбачення нових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_linear(X, y, x_pred, verbose=False):\n",
    "    weights = get_example_weights(X, x_pred, tau=0.1)\n",
    "    theta, losses = gradient_descent(\n",
    "        X,\n",
    "        y,\n",
    "        weights,\n",
    "        loss_fun=cost_function,\n",
    "        grad_fun=cost_function_gradient,\n",
    "        learning_rate=0.005,\n",
    "        convergence_threshold=0.0001,\n",
    "        max_iters=500,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return predict_linear(theta, x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = X_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   1 Loss: 2051.2757624241217\n",
      "Iteration:   2 Loss: 2050.49351072097\n",
      "Iteration:   3 Loss: 2049.711566651771\n",
      "Iteration:   4 Loss: 2048.9299300955427\n",
      "Iteration:   5 Loss: 2048.1486009313508\n",
      "Iteration:   6 Loss: 2047.3675790383077\n",
      "Iteration:   7 Loss: 2046.586864295574\n",
      "Iteration:   8 Loss: 2045.806456582357\n",
      "Iteration:   9 Loss: 2045.0263557779133\n",
      "Iteration:  10 Loss: 2044.2465617615442\n",
      "Iteration:  11 Loss: 2043.4670744126008\n",
      "Iteration:  12 Loss: 2042.6878936104808\n",
      "Iteration:  13 Loss: 2041.9090192346293\n",
      "Iteration:  14 Loss: 2041.1304511645387\n",
      "Iteration:  15 Loss: 2040.3521892797496\n",
      "Iteration:  16 Loss: 2039.574233459848\n",
      "Iteration:  17 Loss: 2038.7965835844707\n",
      "Iteration:  18 Loss: 2038.019239533298\n",
      "Iteration:  19 Loss: 2037.2422011860608\n",
      "Iteration:  20 Loss: 2036.4654684225336\n",
      "Iteration:  21 Loss: 2035.6890411225418\n",
      "Iteration:  22 Loss: 2034.912919165957\n",
      "Iteration:  23 Loss: 2034.1371024326966\n",
      "Iteration:  24 Loss: 2033.3615908027273\n",
      "Iteration:  25 Loss: 2032.5863841560608\n",
      "Iteration:  26 Loss: 2031.8114823727578\n",
      "Iteration:  27 Loss: 2031.0368853329257\n",
      "Iteration:  28 Loss: 2030.262592916719\n",
      "Iteration:  29 Loss: 2029.4886050043383\n",
      "Iteration:  30 Loss: 2028.7149214760327\n",
      "Iteration:  31 Loss: 2027.941542212098\n",
      "Iteration:  32 Loss: 2027.1684670928773\n",
      "Iteration:  33 Loss: 2026.3956959987609\n",
      "Iteration:  34 Loss: 2025.623228810184\n",
      "Iteration:  35 Loss: 2024.8510654076317\n",
      "Iteration:  36 Loss: 2024.0792056716343\n",
      "Iteration:  37 Loss: 2023.307649482771\n",
      "Iteration:  38 Loss: 2022.5363967216651\n",
      "Iteration:  39 Loss: 2021.7654472689892\n",
      "Iteration:  40 Loss: 2020.9948010054625\n",
      "Iteration:  41 Loss: 2020.2244578118493\n",
      "Iteration:  42 Loss: 2019.4544175689632\n",
      "Iteration:  43 Loss: 2018.6846801576628\n",
      "Iteration:  44 Loss: 2017.9152454588557\n",
      "Iteration:  45 Loss: 2017.1461133534935\n",
      "Iteration:  46 Loss: 2016.3772837225772\n",
      "Iteration:  47 Loss: 2015.6087564471534\n",
      "Iteration:  48 Loss: 2014.8405314083145\n",
      "Iteration:  49 Loss: 2014.0726084872017\n",
      "Iteration:  50 Loss: 2013.3049875650017\n",
      "Iteration:  51 Loss: 2012.5376685229487\n",
      "Iteration:  52 Loss: 2011.770651242323\n",
      "Iteration:  53 Loss: 2011.0039356044508\n",
      "Iteration:  54 Loss: 2010.2375214907074\n",
      "Iteration:  55 Loss: 2009.471408782512\n",
      "Iteration:  56 Loss: 2008.7055973613321\n",
      "Iteration:  57 Loss: 2007.9400871086816\n",
      "Iteration:  58 Loss: 2007.174877906121\n",
      "Iteration:  59 Loss: 2006.4099696352564\n",
      "Iteration:  60 Loss: 2005.6453621777416\n",
      "Iteration:  61 Loss: 2004.8810554152767\n",
      "Iteration:  62 Loss: 2004.117049229608\n",
      "Iteration:  63 Loss: 2003.353343502529\n",
      "Iteration:  64 Loss: 2002.5899381158783\n",
      "Iteration:  65 Loss: 2001.8268329515422\n",
      "Iteration:  66 Loss: 2001.0640278914536\n",
      "Iteration:  67 Loss: 2000.3015228175905\n",
      "Iteration:  68 Loss: 1999.5393176119796\n",
      "Iteration:  69 Loss: 1998.7774121566906\n",
      "Iteration:  70 Loss: 1998.015806333843\n",
      "Iteration:  71 Loss: 1997.2545000256002\n",
      "Iteration:  72 Loss: 1996.4934931141738\n",
      "Iteration:  73 Loss: 1995.7327854818202\n",
      "Iteration:  74 Loss: 1994.9723770108433\n",
      "Iteration:  75 Loss: 1994.2122675835917\n",
      "Iteration:  76 Loss: 1993.4524570824633\n",
      "Iteration:  77 Loss: 1992.6929453898981\n",
      "Iteration:  78 Loss: 1991.9337323883856\n",
      "Iteration:  79 Loss: 1991.1748179604597\n",
      "Iteration:  80 Loss: 1990.416201988702\n",
      "Iteration:  81 Loss: 1989.657884355739\n",
      "Iteration:  82 Loss: 1988.8998649442447\n",
      "Iteration:  83 Loss: 1988.142143636937\n",
      "Iteration:  84 Loss: 1987.3847203165822\n",
      "Iteration:  85 Loss: 1986.6275948659911\n",
      "Iteration:  86 Loss: 1985.8707671680222\n",
      "Iteration:  87 Loss: 1985.1142371055778\n",
      "Iteration:  88 Loss: 1984.3580045616097\n",
      "Iteration:  89 Loss: 1983.6020694191118\n",
      "Iteration:  90 Loss: 1982.8464315611266\n",
      "Iteration:  91 Loss: 1982.0910908707415\n",
      "Iteration:  92 Loss: 1981.3360472310906\n",
      "Iteration:  93 Loss: 1980.5813005253528\n",
      "Iteration:  94 Loss: 1979.8268506367554\n",
      "Iteration:  95 Loss: 1979.0726974485685\n",
      "Iteration:  96 Loss: 1978.31884084411\n",
      "Iteration:  97 Loss: 1977.5652807067436\n",
      "Iteration:  98 Loss: 1976.8120169198767\n",
      "Iteration:  99 Loss: 1976.0590493669672\n",
      "Iteration: 100 Loss: 1975.3063779315141\n",
      "Iteration: 101 Loss: 1974.5540024970646\n",
      "Iteration: 102 Loss: 1973.8019229472106\n",
      "Iteration: 103 Loss: 1973.0501391655916\n",
      "Iteration: 104 Loss: 1972.2986510358908\n",
      "Iteration: 105 Loss: 1971.547458441838\n",
      "Iteration: 106 Loss: 1970.7965612672097\n",
      "Iteration: 107 Loss: 1970.0459593958255\n",
      "Iteration: 108 Loss: 1969.2956527115534\n",
      "Iteration: 109 Loss: 1968.5456410983059\n",
      "Iteration: 110 Loss: 1967.7959244400413\n",
      "Iteration: 111 Loss: 1967.0465026207626\n",
      "Iteration: 112 Loss: 1966.2973755245205\n",
      "Iteration: 113 Loss: 1965.548543035409\n",
      "Iteration: 114 Loss: 1964.8000050375697\n",
      "Iteration: 115 Loss: 1964.0517614151881\n",
      "Iteration: 116 Loss: 1963.3038120524968\n",
      "Iteration: 117 Loss: 1962.5561568337723\n",
      "Iteration: 118 Loss: 1961.8087956433378\n",
      "Iteration: 119 Loss: 1961.0617283655615\n",
      "Iteration: 120 Loss: 1960.3149548848576\n",
      "Iteration: 121 Loss: 1959.5684750856847\n",
      "Iteration: 122 Loss: 1958.8222888525481\n",
      "Iteration: 123 Loss: 1958.0763960699971\n",
      "Iteration: 124 Loss: 1957.3307966226284\n",
      "Iteration: 125 Loss: 1956.5854903950815\n",
      "Iteration: 126 Loss: 1955.840477272044\n",
      "Iteration: 127 Loss: 1955.0957571382457\n",
      "Iteration: 128 Loss: 1954.3513298784658\n",
      "Iteration: 129 Loss: 1953.6071953775245\n",
      "Iteration: 130 Loss: 1952.8633535202907\n",
      "Iteration: 131 Loss: 1952.1198041916766\n",
      "Iteration: 132 Loss: 1951.3765472766413\n",
      "Iteration: 133 Loss: 1950.6335826601864\n",
      "Iteration: 134 Loss: 1949.890910227361\n",
      "Iteration: 135 Loss: 1949.14852986326\n",
      "Iteration: 136 Loss: 1948.4064414530217\n",
      "Iteration: 137 Loss: 1947.6646448818296\n",
      "Iteration: 138 Loss: 1946.923140034913\n",
      "Iteration: 139 Loss: 1946.1819267975477\n",
      "Iteration: 140 Loss: 1945.4410050550514\n",
      "Iteration: 141 Loss: 1944.7003746927899\n",
      "Iteration: 142 Loss: 1943.960035596173\n",
      "Iteration: 143 Loss: 1943.219987650654\n",
      "Iteration: 144 Loss: 1942.4802307417344\n",
      "Iteration: 145 Loss: 1941.7407647549587\n",
      "Iteration: 146 Loss: 1941.0015895759166\n",
      "Iteration: 147 Loss: 1940.2627050902422\n",
      "Iteration: 148 Loss: 1939.5241111836162\n",
      "Iteration: 149 Loss: 1938.7858077417627\n",
      "Iteration: 150 Loss: 1938.0477946504527\n",
      "Iteration: 151 Loss: 1937.3100717954994\n",
      "Iteration: 152 Loss: 1936.572639062763\n",
      "Iteration: 153 Loss: 1935.835496338148\n",
      "Iteration: 154 Loss: 1935.0986435076038\n",
      "Iteration: 155 Loss: 1934.3620804571244\n",
      "Iteration: 156 Loss: 1933.6258070727479\n",
      "Iteration: 157 Loss: 1932.8898232405597\n",
      "Iteration: 158 Loss: 1932.154128846688\n",
      "Iteration: 159 Loss: 1931.418723777306\n",
      "Iteration: 160 Loss: 1930.6836079186307\n",
      "Iteration: 161 Loss: 1929.9487811569268\n",
      "Iteration: 162 Loss: 1929.2142433785011\n",
      "Iteration: 163 Loss: 1928.479994469706\n",
      "Iteration: 164 Loss: 1927.7460343169387\n",
      "Iteration: 165 Loss: 1927.0123628066417\n",
      "Iteration: 166 Loss: 1926.2789798252993\n",
      "Iteration: 167 Loss: 1925.5458852594438\n",
      "Iteration: 168 Loss: 1924.813078995651\n",
      "Iteration: 169 Loss: 1924.0805609205413\n",
      "Iteration: 170 Loss: 1923.3483309207788\n",
      "Iteration: 171 Loss: 1922.6163888830733\n",
      "Iteration: 172 Loss: 1921.8847346941789\n",
      "Iteration: 173 Loss: 1921.153368240894\n",
      "Iteration: 174 Loss: 1920.42228941006\n",
      "Iteration: 175 Loss: 1919.691498088568\n",
      "Iteration: 176 Loss: 1918.9609941633476\n",
      "Iteration: 177 Loss: 1918.230777521375\n",
      "Iteration: 178 Loss: 1917.5008480496717\n",
      "Iteration: 179 Loss: 1916.7712056353037\n",
      "Iteration: 180 Loss: 1916.0418501653796\n",
      "Iteration: 181 Loss: 1915.3127815270545\n",
      "Iteration: 182 Loss: 1914.5839996075263\n",
      "Iteration: 183 Loss: 1913.8555042940382\n",
      "Iteration: 184 Loss: 1913.1272954738772\n",
      "Iteration: 185 Loss: 1912.399373034375\n",
      "Iteration: 186 Loss: 1911.671736862908\n",
      "Iteration: 187 Loss: 1910.9443868468954\n",
      "Iteration: 188 Loss: 1910.2173228738025\n",
      "Iteration: 189 Loss: 1909.4905448311376\n",
      "Iteration: 190 Loss: 1908.7640526064529\n",
      "Iteration: 191 Loss: 1908.0378460873474\n",
      "Iteration: 192 Loss: 1907.3119251614605\n",
      "Iteration: 193 Loss: 1906.5862897164784\n",
      "Iteration: 194 Loss: 1905.8609396401323\n",
      "Iteration: 195 Loss: 1905.135874820194\n",
      "Iteration: 196 Loss: 1904.4110951444825\n",
      "Iteration: 197 Loss: 1903.6866005008594\n",
      "Iteration: 198 Loss: 1902.9623907772316\n",
      "Iteration: 199 Loss: 1902.238465861549\n",
      "Iteration: 200 Loss: 1901.5148256418056\n",
      "Iteration: 201 Loss: 1900.791470006041\n",
      "Iteration: 202 Loss: 1900.0683988423368\n",
      "Iteration: 203 Loss: 1899.345612038819\n",
      "Iteration: 204 Loss: 1898.623109483659\n",
      "Iteration: 205 Loss: 1897.9008910650705\n",
      "Iteration: 206 Loss: 1897.1789566713128\n",
      "Iteration: 207 Loss: 1896.4573061906872\n",
      "Iteration: 208 Loss: 1895.7359395115407\n",
      "Iteration: 209 Loss: 1895.0148565222632\n",
      "Iteration: 210 Loss: 1894.294057111289\n",
      "Iteration: 211 Loss: 1893.5735411670953\n",
      "Iteration: 212 Loss: 1892.8533085782053\n",
      "Iteration: 213 Loss: 1892.1333592331835\n",
      "Iteration: 214 Loss: 1891.4136930206394\n",
      "Iteration: 215 Loss: 1890.6943098292274\n",
      "Iteration: 216 Loss: 1889.975209547643\n",
      "Iteration: 217 Loss: 1889.2563920646278\n",
      "Iteration: 218 Loss: 1888.5378572689667\n",
      "Iteration: 219 Loss: 1887.8196050494876\n",
      "Iteration: 220 Loss: 1887.1016352950621\n",
      "Iteration: 221 Loss: 1886.3839478946081\n",
      "Iteration: 222 Loss: 1885.6665427370824\n",
      "Iteration: 223 Loss: 1884.9494197114886\n",
      "Iteration: 224 Loss: 1884.2325787068746\n",
      "Iteration: 225 Loss: 1883.5160196123304\n",
      "Iteration: 226 Loss: 1882.7997423169898\n",
      "Iteration: 227 Loss: 1882.0837467100298\n",
      "Iteration: 228 Loss: 1881.3680326806732\n",
      "Iteration: 229 Loss: 1880.652600118183\n",
      "Iteration: 230 Loss: 1879.9374489118684\n",
      "Iteration: 231 Loss: 1879.2225789510812\n",
      "Iteration: 232 Loss: 1878.507990125217\n",
      "Iteration: 233 Loss: 1877.7936823237144\n",
      "Iteration: 234 Loss: 1877.0796554360547\n",
      "Iteration: 235 Loss: 1876.3659093517658\n",
      "Iteration: 236 Loss: 1875.6524439604148\n",
      "Iteration: 237 Loss: 1874.9392591516164\n",
      "Iteration: 238 Loss: 1874.226354815025\n",
      "Iteration: 239 Loss: 1873.5137308403405\n",
      "Iteration: 240 Loss: 1872.801387117307\n",
      "Iteration: 241 Loss: 1872.0893235357084\n",
      "Iteration: 242 Loss: 1871.3775399853764\n",
      "Iteration: 243 Loss: 1870.666036356183\n",
      "Iteration: 244 Loss: 1869.9548125380443\n",
      "Iteration: 245 Loss: 1869.2438684209194\n",
      "Iteration: 246 Loss: 1868.5332038948118\n",
      "Iteration: 247 Loss: 1867.8228188497676\n",
      "Iteration: 248 Loss: 1867.1127131758756\n",
      "Iteration: 249 Loss: 1866.4028867632678\n",
      "Iteration: 250 Loss: 1865.6933395021215\n",
      "Iteration: 251 Loss: 1864.9840712826538\n",
      "Iteration: 252 Loss: 1864.2750819951284\n",
      "Iteration: 253 Loss: 1863.566371529849\n",
      "Iteration: 254 Loss: 1862.8579397771646\n",
      "Iteration: 255 Loss: 1862.1497866274674\n",
      "Iteration: 256 Loss: 1861.4419119711902\n",
      "Iteration: 257 Loss: 1860.7343156988125\n",
      "Iteration: 258 Loss: 1860.0269977008538\n",
      "Iteration: 259 Loss: 1859.3199578678784\n",
      "Iteration: 260 Loss: 1858.6131960904931\n",
      "Iteration: 261 Loss: 1857.9067122593478\n",
      "Iteration: 262 Loss: 1857.2005062651353\n",
      "Iteration: 263 Loss: 1856.494577998591\n",
      "Iteration: 264 Loss: 1855.7889273504948\n",
      "Iteration: 265 Loss: 1855.083554211668\n",
      "Iteration: 266 Loss: 1854.3784584729744\n",
      "Iteration: 267 Loss: 1853.6736400253224\n",
      "Iteration: 268 Loss: 1852.969098759663\n",
      "Iteration: 269 Loss: 1852.2648345669888\n",
      "Iteration: 270 Loss: 1851.5608473383354\n",
      "Iteration: 271 Loss: 1850.857136964784\n",
      "Iteration: 272 Loss: 1850.1537033374555\n",
      "Iteration: 273 Loss: 1849.450546347514\n",
      "Iteration: 274 Loss: 1848.7476658861683\n",
      "Iteration: 275 Loss: 1848.0450618446675\n",
      "Iteration: 276 Loss: 1847.3427341143056\n",
      "Iteration: 277 Loss: 1846.6406825864185\n",
      "Iteration: 278 Loss: 1845.9389071523835\n",
      "Iteration: 279 Loss: 1845.2374077036247\n",
      "Iteration: 280 Loss: 1844.536184131604\n",
      "Iteration: 281 Loss: 1843.8352363278282\n",
      "Iteration: 282 Loss: 1843.134564183847\n",
      "Iteration: 283 Loss: 1842.434167591253\n",
      "Iteration: 284 Loss: 1841.7340464416807\n",
      "Iteration: 285 Loss: 1841.0342006268063\n",
      "Iteration: 286 Loss: 1840.3346300383507\n",
      "Iteration: 287 Loss: 1839.635334568076\n",
      "Iteration: 288 Loss: 1838.9363141077877\n",
      "Iteration: 289 Loss: 1838.2375685493325\n",
      "Iteration: 290 Loss: 1837.5390977846016\n",
      "Iteration: 291 Loss: 1836.8409017055271\n",
      "Iteration: 292 Loss: 1836.142980204084\n",
      "Iteration: 293 Loss: 1835.4453331722907\n",
      "Iteration: 294 Loss: 1834.747960502206\n",
      "Iteration: 295 Loss: 1834.0508620859327\n",
      "Iteration: 296 Loss: 1833.3540378156172\n",
      "Iteration: 297 Loss: 1832.6574875834453\n",
      "Iteration: 298 Loss: 1831.9612112816471\n",
      "Iteration: 299 Loss: 1831.2652088024954\n",
      "Iteration: 300 Loss: 1830.5694800383042\n",
      "Iteration: 301 Loss: 1829.8740248814308\n",
      "Iteration: 302 Loss: 1829.1788432242734\n",
      "Iteration: 303 Loss: 1828.4839349592746\n",
      "Iteration: 304 Loss: 1827.7892999789183\n",
      "Iteration: 305 Loss: 1827.0949381757298\n",
      "Iteration: 306 Loss: 1826.4008494422776\n",
      "Iteration: 307 Loss: 1825.707033671173\n",
      "Iteration: 308 Loss: 1825.0134907550678\n",
      "Iteration: 309 Loss: 1824.320220586658\n",
      "Iteration: 310 Loss: 1823.6272230586806\n",
      "Iteration: 311 Loss: 1822.9344980639155\n",
      "Iteration: 312 Loss: 1822.2420454951832\n",
      "Iteration: 313 Loss: 1821.5498652453484\n",
      "Iteration: 314 Loss: 1820.8579572073165\n",
      "Iteration: 315 Loss: 1820.1663212740357\n",
      "Iteration: 316 Loss: 1819.474957338496\n",
      "Iteration: 317 Loss: 1818.78386529373\n",
      "Iteration: 318 Loss: 1818.0930450328124\n",
      "Iteration: 319 Loss: 1817.402496448858\n",
      "Iteration: 320 Loss: 1816.7122194350266\n",
      "Iteration: 321 Loss: 1816.022213884517\n",
      "Iteration: 322 Loss: 1815.3324796905738\n",
      "Iteration: 323 Loss: 1814.6430167464796\n",
      "Iteration: 324 Loss: 1813.9538249455609\n",
      "Iteration: 325 Loss: 1813.2649041811867\n",
      "Iteration: 326 Loss: 1812.576254346766\n",
      "Iteration: 327 Loss: 1811.887875335753\n",
      "Iteration: 328 Loss: 1811.19976704164\n",
      "Iteration: 329 Loss: 1810.5119293579628\n",
      "Iteration: 330 Loss: 1809.8243621783006\n",
      "Iteration: 331 Loss: 1809.137065396272\n",
      "Iteration: 332 Loss: 1808.4500389055386\n",
      "Iteration: 333 Loss: 1807.7632825998041\n",
      "Iteration: 334 Loss: 1807.0767963728129\n",
      "Iteration: 335 Loss: 1806.390580118352\n",
      "Iteration: 336 Loss: 1805.704633730251\n",
      "Iteration: 337 Loss: 1805.01895710238\n",
      "Iteration: 338 Loss: 1804.33355012865\n",
      "Iteration: 339 Loss: 1803.6484127030153\n",
      "Iteration: 340 Loss: 1802.9635447194732\n",
      "Iteration: 341 Loss: 1802.2789460720578\n",
      "Iteration: 342 Loss: 1801.594616654851\n",
      "Iteration: 343 Loss: 1800.9105563619723\n",
      "Iteration: 344 Loss: 1800.2267650875835\n",
      "Iteration: 345 Loss: 1799.5432427258886\n",
      "Iteration: 346 Loss: 1798.8599891711335\n",
      "Iteration: 347 Loss: 1798.1770043176043\n",
      "Iteration: 348 Loss: 1797.4942880596307\n",
      "Iteration: 349 Loss: 1796.8118402915825\n",
      "Iteration: 350 Loss: 1796.129660907871\n",
      "Iteration: 351 Loss: 1795.44774980295\n",
      "Iteration: 352 Loss: 1794.7661068713142\n",
      "Iteration: 353 Loss: 1794.0847320074997\n",
      "Iteration: 354 Loss: 1793.4036251060838\n",
      "Iteration: 355 Loss: 1792.722786061687\n",
      "Iteration: 356 Loss: 1792.0422147689683\n",
      "Iteration: 357 Loss: 1791.3619111226303\n",
      "Iteration: 358 Loss: 1790.6818750174166\n",
      "Iteration: 359 Loss: 1790.0021063481115\n",
      "Iteration: 360 Loss: 1789.322605009543\n",
      "Iteration: 361 Loss: 1788.6433708965765\n",
      "Iteration: 362 Loss: 1787.9644039041223\n",
      "Iteration: 363 Loss: 1787.2857039271298\n",
      "Iteration: 364 Loss: 1786.6072708605914\n",
      "Iteration: 365 Loss: 1785.9291045995392\n",
      "Iteration: 366 Loss: 1785.2512050390478\n",
      "Iteration: 367 Loss: 1784.573572074232\n",
      "Iteration: 368 Loss: 1783.89620560025\n",
      "Iteration: 369 Loss: 1783.219105512298\n",
      "Iteration: 370 Loss: 1782.5422717056163\n",
      "Iteration: 371 Loss: 1781.8657040754842\n",
      "Iteration: 372 Loss: 1781.1894025172235\n",
      "Iteration: 373 Loss: 1780.513366926197\n",
      "Iteration: 374 Loss: 1779.8375971978094\n",
      "Iteration: 375 Loss: 1779.1620932275039\n",
      "Iteration: 376 Loss: 1778.4868549107678\n",
      "Iteration: 377 Loss: 1777.8118821431278\n",
      "Iteration: 378 Loss: 1777.1371748201516\n",
      "Iteration: 379 Loss: 1776.4627328374495\n",
      "Iteration: 380 Loss: 1775.788556090672\n",
      "Iteration: 381 Loss: 1775.1146444755095\n",
      "Iteration: 382 Loss: 1774.4409978876952\n",
      "Iteration: 383 Loss: 1773.767616223002\n",
      "Iteration: 384 Loss: 1773.0944993772446\n",
      "Iteration: 385 Loss: 1772.4216472462788\n",
      "Iteration: 386 Loss: 1771.7490597260003\n",
      "Iteration: 387 Loss: 1771.0767367123462\n",
      "Iteration: 388 Loss: 1770.4046781012955\n",
      "Iteration: 389 Loss: 1769.7328837888674\n",
      "Iteration: 390 Loss: 1769.0613536711205\n",
      "Iteration: 391 Loss: 1768.390087644157\n",
      "Iteration: 392 Loss: 1767.7190856041186\n",
      "Iteration: 393 Loss: 1767.0483474471876\n",
      "Iteration: 394 Loss: 1766.3778730695872\n",
      "Iteration: 395 Loss: 1765.7076623675816\n",
      "Iteration: 396 Loss: 1765.0377152374767\n",
      "Iteration: 397 Loss: 1764.3680315756176\n",
      "Iteration: 398 Loss: 1763.6986112783907\n",
      "Iteration: 399 Loss: 1763.0294542422241\n",
      "Iteration: 400 Loss: 1762.3605603635847\n",
      "Iteration: 401 Loss: 1761.691929538982\n",
      "Iteration: 402 Loss: 1761.0235616649659\n",
      "Iteration: 403 Loss: 1760.355456638126\n",
      "Iteration: 404 Loss: 1759.687614355093\n",
      "Iteration: 405 Loss: 1759.020034712538\n",
      "Iteration: 406 Loss: 1758.3527176071746\n",
      "Iteration: 407 Loss: 1757.6856629357544\n",
      "Iteration: 408 Loss: 1757.0188705950704\n",
      "Iteration: 409 Loss: 1756.3523404819573\n",
      "Iteration: 410 Loss: 1755.686072493289\n",
      "Iteration: 411 Loss: 1755.0200665259815\n",
      "Iteration: 412 Loss: 1754.3543224769899\n",
      "Iteration: 413 Loss: 1753.6888402433096\n",
      "Iteration: 414 Loss: 1753.0236197219779\n",
      "Iteration: 415 Loss: 1752.358660810072\n",
      "Iteration: 416 Loss: 1751.6939634047096\n",
      "Iteration: 417 Loss: 1751.0295274030475\n",
      "Iteration: 418 Loss: 1750.3653527022864\n",
      "Iteration: 419 Loss: 1749.7014391996627\n",
      "Iteration: 420 Loss: 1749.0377867924574\n",
      "Iteration: 421 Loss: 1748.3743953779901\n",
      "Iteration: 422 Loss: 1747.7112648536208\n",
      "Iteration: 423 Loss: 1747.04839511675\n",
      "Iteration: 424 Loss: 1746.385786064817\n",
      "Iteration: 425 Loss: 1745.7234375953053\n",
      "Iteration: 426 Loss: 1745.0613496057354\n",
      "Iteration: 427 Loss: 1744.3995219936692\n",
      "Iteration: 428 Loss: 1743.737954656708\n",
      "Iteration: 429 Loss: 1743.0766474924953\n",
      "Iteration: 430 Loss: 1742.415600398713\n",
      "Iteration: 431 Loss: 1741.7548132730835\n",
      "Iteration: 432 Loss: 1741.0942860133716\n",
      "Iteration: 433 Loss: 1740.4340185173783\n",
      "Iteration: 434 Loss: 1739.7740106829488\n",
      "Iteration: 435 Loss: 1739.1142624079653\n",
      "Iteration: 436 Loss: 1738.4547735903525\n",
      "Iteration: 437 Loss: 1737.7955441280747\n",
      "Iteration: 438 Loss: 1737.136573919135\n",
      "Iteration: 439 Loss: 1736.4778628615777\n",
      "Iteration: 440 Loss: 1735.8194108534872\n",
      "Iteration: 441 Loss: 1735.1612177929885\n",
      "Iteration: 442 Loss: 1734.5032835782454\n",
      "Iteration: 443 Loss: 1733.845608107462\n",
      "Iteration: 444 Loss: 1733.1881912788833\n",
      "Iteration: 445 Loss: 1732.5310329907934\n",
      "Iteration: 446 Loss: 1731.8741331415174\n",
      "Iteration: 447 Loss: 1731.2174916294193\n",
      "Iteration: 448 Loss: 1730.5611083529036\n",
      "Iteration: 449 Loss: 1729.9049832104154\n",
      "Iteration: 450 Loss: 1729.249116100437\n",
      "Iteration: 451 Loss: 1728.5935069214956\n",
      "Iteration: 452 Loss: 1727.9381555721532\n",
      "Iteration: 453 Loss: 1727.2830619510146\n",
      "Iteration: 454 Loss: 1726.6282259567238\n",
      "Iteration: 455 Loss: 1725.9736474879649\n",
      "Iteration: 456 Loss: 1725.3193264434606\n",
      "Iteration: 457 Loss: 1724.6652627219755\n",
      "Iteration: 458 Loss: 1724.0114562223116\n",
      "Iteration: 459 Loss: 1723.3579068433137\n",
      "Iteration: 460 Loss: 1722.7046144838625\n",
      "Iteration: 461 Loss: 1722.0515790428826\n",
      "Iteration: 462 Loss: 1721.3988004193359\n",
      "Iteration: 463 Loss: 1720.746278512224\n",
      "Iteration: 464 Loss: 1720.0940132205887\n",
      "Iteration: 465 Loss: 1719.442004443512\n",
      "Iteration: 466 Loss: 1718.790252080115\n",
      "Iteration: 467 Loss: 1718.1387560295589\n",
      "Iteration: 468 Loss: 1717.4875161910436\n",
      "Iteration: 469 Loss: 1716.8365324638096\n",
      "Iteration: 470 Loss: 1716.1858047471367\n",
      "Iteration: 471 Loss: 1715.5353329403445\n",
      "Iteration: 472 Loss: 1714.8851169427921\n",
      "Iteration: 473 Loss: 1714.2351566538778\n",
      "Iteration: 474 Loss: 1713.5854519730394\n",
      "Iteration: 475 Loss: 1712.936002799756\n",
      "Iteration: 476 Loss: 1712.286809033544\n",
      "Iteration: 477 Loss: 1711.6378705739596\n",
      "Iteration: 478 Loss: 1710.9891873205997\n",
      "Iteration: 479 Loss: 1710.3407591731002\n",
      "Iteration: 480 Loss: 1709.692586031136\n",
      "Iteration: 481 Loss: 1709.0446677944221\n",
      "Iteration: 482 Loss: 1708.3970043627126\n",
      "Iteration: 483 Loss: 1707.7495956358002\n",
      "Iteration: 484 Loss: 1707.102441513519\n",
      "Iteration: 485 Loss: 1706.4555418957407\n",
      "Iteration: 486 Loss: 1705.8088966823775\n",
      "Iteration: 487 Loss: 1705.16250577338\n",
      "Iteration: 488 Loss: 1704.5163690687386\n",
      "Iteration: 489 Loss: 1703.8704864684835\n",
      "Iteration: 490 Loss: 1703.2248578726835\n",
      "Iteration: 491 Loss: 1702.5794831814467\n",
      "Iteration: 492 Loss: 1701.9343622949216\n",
      "Iteration: 493 Loss: 1701.2894951132942\n",
      "Iteration: 494 Loss: 1700.6448815367912\n",
      "Iteration: 495 Loss: 1700.000521465678\n",
      "Iteration: 496 Loss: 1699.3564148002588\n",
      "Iteration: 497 Loss: 1698.712561440878\n",
      "Iteration: 498 Loss: 1698.0689612879187\n",
      "Iteration: 499 Loss: 1697.4256142418021\n",
      "Iteration: 500 Loss: 1696.7825202029908\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_weighted_linear(X_train, y_train, x_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцінимо, наскільки модель помиляється на тестовій вибірці:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.array([predict_weighted_linear(X_test, y_test, X_test[i]) for i in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame(y_test - y_test_pred, columns=[\"residual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5,1,'Error Distribution')]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df_residuals.plot.bar(figsize=(10, 5))\n",
    "ax.legend().remove()\n",
    "ax.get_xaxis().set(ticklabels=[])\n",
    "ax.set(xlabel=\"Example #\")\n",
    "ax.set(ylabel=\"Prediction Error, $\")\n",
    "ax.set(title=\"Residuals\")\n",
    "\n",
    "bins = np.arange(-3000, 3001, 100)\n",
    "ticks = np.arange(-3000, 3001, 500)\n",
    "\n",
    "ax = df_residuals.hist(bins=bins, figsize=(10, 5))[0][0]\n",
    "ax.set(xticks=ticks)\n",
    "ax.set(xlabel=\"Prediction Error, $\")\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(title=\"Error Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестування"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустіть комірку нижче, щоб перевірити правильність вашого коду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "salmon",
       "message": "..FF........\n======================================================================\nFAIL: test_cost_gradient_should_compute_correct_gradient_unweighted (__main__.WLRTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 63, in test_cost_gradient_should_compute_correct_gradient_unweighted\n  File \"Cell Tests\", line 21, in assertArrayEquals\nAssertionError: False is not true\n\n======================================================================\nFAIL: test_cost_gradient_should_compute_correct_gradient_weighted (__main__.WLRTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 69, in test_cost_gradient_should_compute_correct_gradient_weighted\n  File \"Cell Tests\", line 21, in assertArrayEquals\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 12 tests in 0.002s\n\nFAILED (failures=2)\n",
       "previous": 0
      },
      "text/plain": [
       "Fail"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..FF........\n",
      "======================================================================\n",
      "FAIL: test_cost_gradient_should_compute_correct_gradient_unweighted (__main__.WLRTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 63, in test_cost_gradient_should_compute_correct_gradient_unweighted\n",
      "  File \"Cell Tests\", line 21, in assertArrayEquals\n",
      "AssertionError: False is not true\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_cost_gradient_should_compute_correct_gradient_weighted (__main__.WLRTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"Cell Tests\", line 69, in test_cost_gradient_should_compute_correct_gradient_weighted\n",
      "  File \"Cell Tests\", line 21, in assertArrayEquals\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.002s\n",
      "\n",
      "FAILED (failures=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=2>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "\n",
    "class WLRTests(unittest.TestCase):\n",
    "\n",
    "    x = np.array([1, -0.5, 3, 1])\n",
    "    X = np.array([\n",
    "        [1, -0.5, 3, 1],\n",
    "        [2, 8, -0.33, 5],\n",
    "        [0, 0, 0, 0]\n",
    "    ])\n",
    "    y = np.array([40, 100, 12])\n",
    "    theta = np.array([2, 5, 7, 9])\n",
    "    eps = 0.001\n",
    "\n",
    "    def assertFloatEquals(self, a, b):\n",
    "        self.assertTrue(np.abs(a - b) < self.eps)\n",
    "    \n",
    "    def assertArrayEquals(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        self.assertEqual(a.shape, b.shape)\n",
    "        self.assertTrue(np.all(np.abs(a - b) < self.eps))\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_prediction_for_1_example(self):\n",
    "        expected = 29.5\n",
    "        actual = predict_linear(self.theta, self.x)\n",
    "        self.assertEqual(actual, expected)\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_predictions_for_multiple_examples(self):\n",
    "        expected = [29.5, 86.69, 0]\n",
    "        actual = (predict_linear(self.theta, self.X))\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_get_example_weights_should_return_properly_shaped_vector(self):\n",
    "        weights = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertTrue(weights.shape[0] == self.X.shape[0])\n",
    "    \n",
    "    def test_get_example_weights_should_compute_correct_weights(self):\n",
    "        expected = [1.000, 0.134, 0.798]\n",
    "        actual = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_function_should_compute_correct_cost_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = 71.901\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_function_should_compute_correct_cost_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = 18.860\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "\n",
    "    def test_cost_gradient_should_return_properly_shaped_vector(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        grad = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertTrue(grad.shape == self.theta.shape)\n",
    "        \n",
    "    def test_cost_gradient_should_compute_correct_gradient_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = [-37.12, -101.23, -27.108, -77.05]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_cost_gradient_should_compute_correct_gradient_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = [-7.912, -8.023, -15.311, -11.905]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_update_model_weights_should_not_update_when_gradient_is_zero(self):\n",
    "        grad = np.zeros(self.theta.shape[0])\n",
    "        theta_new = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(theta_new, self.theta)\n",
    "    \n",
    "    def test_update_model_weights_should_update_with_complete_gradient_if_learning_rate_is_one(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [0.65, 5.89, 6.84, 8.02]\n",
    "        actual = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_update_model_weights_should_take_learning_rate_into_account(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [1.730, 5.178, 6.968, 8.804]\n",
    "        actual = update_model_weights(self.theta, learning_rate=0.2, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
